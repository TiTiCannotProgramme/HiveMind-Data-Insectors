{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccfc9e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import sys # Python system library needed to load custom functions\n",
    "import math # module with access to mathematical functions\n",
    "import os # for changing the directory\n",
    "\n",
    "import numpy as np  # for performing calculations on numerical arrays\n",
    "import pandas as pd  # home of the DataFrame construct, _the_ most important object for Data Science\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt  # allows creation of insightful plots\n",
    "\n",
    "import librosa\n",
    "\n",
    "sys.path.append('../../audio_preprocessing')\n",
    "sys.path.append('../../src')\n",
    "sys.path.append('../../model_training_utils')\n",
    "\n",
    "\n",
    "import preprocessing_func_3\n",
    "from generator_to_dataset_3 import NormalisedDataSet\n",
    "from gdsc_utils import PROJECT_DIR\n",
    "import model_training\n",
    "import model_eval\n",
    "\n",
    "os.chdir(PROJECT_DIR) # changing our directory to root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43bcae9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>file_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66835</th>\n",
       "      <td>44959</td>\n",
       "      <td>data/big_data_upsample/44959.wav</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66836</th>\n",
       "      <td>44960</td>\n",
       "      <td>data/big_data_upsample/44960.wav</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66837</th>\n",
       "      <td>44961</td>\n",
       "      <td>data/big_data_upsample/44961.wav</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66838</th>\n",
       "      <td>44962</td>\n",
       "      <td>data/big_data_upsample/44962.wav</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66839</th>\n",
       "      <td>44963</td>\n",
       "      <td>data/big_data_upsample/44963.wav</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                         file_path  label\n",
       "66835       44959  data/big_data_upsample/44959.wav     65\n",
       "66836       44960  data/big_data_upsample/44960.wav     65\n",
       "66837       44961  data/big_data_upsample/44961.wav     65\n",
       "66838       44962  data/big_data_upsample/44962.wav     65\n",
       "66839       44963  data/big_data_upsample/44963.wav     65"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_big_data = pd.read_csv('data/big_processed_data.csv')\n",
    "df_big_argumented_data = pd.read_csv('data/big_argumentation_data.csv')\n",
    "df = pd.concat([df_big_data, df_big_argumented_data], ignore_index=True)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae80c957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open('audio_preprocessing/saved_data/new_data.json') as f:\n",
    "#     my_info = json.load(f)\n",
    "\n",
    "# mean, std, class_weights = my_info[\"mean\"], my_info[\"std\"], my_info[\"weights\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f67d2545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def resize_function(output_shape=(40,256)):\n",
    "#     return torchvision.transforms.Resize(size=output_shape, antialias=False)\n",
    "\n",
    "# def calculate_melsp(x, n_fft=1024, hop_length=128):\n",
    "#     stft = np.abs(librosa.stft(x, n_fft=n_fft, hop_length=hop_length))**2\n",
    "#     log_stft = librosa.power_to_db(stft)\n",
    "#     melsp = librosa.feature.melspectrogram(S=log_stft,n_mels=40)\n",
    "#     return melsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3653f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_list = []\n",
    "val_df_list = []\n",
    "\n",
    "for i in range(66):\n",
    "    my_df = df[df[\"label\"] == i]\n",
    "    current_train_df, current_val_df = train_test_split(my_df, test_size=0.2)\n",
    "    train_df_list.append(current_train_df)\n",
    "    val_df_list.append(current_val_df)\n",
    "\n",
    "df_train = pd.concat(train_df_list, ignore_index=True)\n",
    "df_val = pd.concat(val_df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0acefaf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((53441, 3), (13399, 3))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75d494fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths, labels = list(df_train[\"file_path\"]), list(df_train[\"label\"])\n",
    "\n",
    "non_normal_gen = preprocessing_func_3.non_normalised_data_generator(\n",
    "    paths=paths,\n",
    "    labels=labels,\n",
    "#     image_preprocess_fn=resize_function(output_shape=(40,256)),\n",
    "#     mel_transform_fn=calculate_melsp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2081d8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std, class_wights = preprocessing_func_3.get_stats_and_class_weights_of_non_normalised_data_gen(\n",
    "    non_normal_gen, (128, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46747625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1.0922]),\n",
       " tensor([0.7937]),\n",
       " array([1.01087656, 1.01214015, 1.00961611, 1.00961611, 1.00961611,\n",
       "        1.01087656, 1.01087656, 1.01087656, 1.01214015, 1.01214015,\n",
       "        1.01214015, 1.00961611, 1.01214015, 1.01087656, 1.00961611,\n",
       "        1.01214015, 1.01087656, 1.01214015, 1.00961611, 0.81954668,\n",
       "        1.01214015, 1.01087656, 1.01214015, 1.01087656, 1.01214015,\n",
       "        1.00710463, 1.00961611, 1.00961611, 1.01087656, 0.6747601 ,\n",
       "        1.01214015, 1.00961611, 1.01214015, 1.01214015, 1.00961611,\n",
       "        1.01087656, 1.01214015, 1.00961611, 1.01214015, 1.01087656,\n",
       "        1.01214015, 1.01214015, 1.01214015, 1.01087656, 1.00961611,\n",
       "        1.01214015, 1.01087656, 1.01214015, 1.01087656, 1.01214015,\n",
       "        1.00961611, 1.01214015, 1.01087656, 1.01214015, 1.01214015,\n",
       "        1.01087656, 1.00961611, 1.01087656, 1.01087656, 1.01214015,\n",
       "        1.01214015, 1.01214015, 1.01087656, 1.01087656, 1.01214015,\n",
       "        1.01087656]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, std, class_wights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b87ca42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_as_json(path, description, mean, std, weights):\n",
    "    my_dict = {\n",
    "        \"description\": description,\n",
    "        \"mean\": float(mean),\n",
    "        \"std\": float(std),\n",
    "        \"weights\": list(class_wights.astype(float)),\n",
    "    }\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(my_dict, f)\n",
    "\n",
    "save_as_json(\n",
    "    \"audio_preprocessing/saved_data/upsampled_data_size_128_512.json\", \n",
    "    \"seconds 1.5, image shape (128,512)\", mean, std, class_wights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eae5ad73",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NormalisedDataSet(\n",
    "    non_normalised_data_generator_fn=preprocessing_func_3.non_normalised_data_generator, \n",
    "    normalised_data_generator_fn=preprocessing_func_3.normalised_data_generator,\n",
    "    df=df_train, \n",
    "    mean=mean,\n",
    "    std=std,\n",
    "    shuffle=True,\n",
    "#     image_preprocess_fn=resize_function(output_shape=(40,256)),\n",
    "#     mel_transform_fn=calculate_melsp,\n",
    ")\n",
    "\n",
    "val_dataset = NormalisedDataSet(\n",
    "    non_normalised_data_generator_fn=preprocessing_func_3.non_normalised_data_generator, \n",
    "    normalised_data_generator_fn=preprocessing_func_3.normalised_data_generator,\n",
    "    df=df_val, \n",
    "    mean=mean,\n",
    "    std=std,\n",
    "    shuffle=False,\n",
    "#     image_preprocess_fn=resize_function(output_shape=(40,256)),\n",
    "#     mel_transform_fn=calculate_melsp,\n",
    ")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=28)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10903070",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = model_training.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cbedc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet34, ResNet34_Weights\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "#resnet_model = resnet34(weights=ResNet34_Weights.DEFAULT)\n",
    "resnet_model = resnet34()\n",
    "resnet_model.fc = nn.Linear(512, 66)\n",
    "resnet_model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "resnet_model = resnet_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "787c4161",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(resnet_model.parameters(), amsgrad=True)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9018df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 0: training accuracy = 69.80%, training loss = 1.0375653201252644, training time taken = 566.14 seconds\n",
      "End of epoch 0: validation accuracy = 85.80%, validation loss = 0.4922554972780249, validation time taken = 204.67 seconds\n",
      "End of epoch 1: training accuracy = 91.09%, training loss = 0.29849428516671633, training time taken = 849.23 seconds\n",
      "End of epoch 1: validation accuracy = 94.50%, validation loss = 0.18138490816456476, validation time taken = 181.40 seconds\n",
      "End of epoch 2: training accuracy = 94.96%, training loss = 0.16582944459895832, training time taken = 849.01 seconds\n",
      "End of epoch 2: validation accuracy = 95.96%, validation loss = 0.1317654311385667, validation time taken = 173.81 seconds\n",
      "End of epoch 3: training accuracy = 96.51%, training loss = 0.11546299124990077, training time taken = 773.79 seconds\n",
      "End of epoch 3: validation accuracy = 97.39%, validation loss = 0.08546473813751072, validation time taken = 176.42 seconds\n",
      "End of epoch 4: training accuracy = 97.51%, training loss = 0.08217882694424174, training time taken = 785.92 seconds\n",
      "End of epoch 4: validation accuracy = 97.68%, validation loss = 0.07896187554291494, validation time taken = 167.31 seconds\n",
      "End of epoch 5: training accuracy = 98.10%, training loss = 0.06246157390866616, training time taken = 803.27 seconds\n",
      "End of epoch 5: validation accuracy = 98.06%, validation loss = 0.0633456484888173, validation time taken = 179.43 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model_training\u001b[38;5;241m.\u001b[39mtraining(\n\u001b[0;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mresnet_model, \n\u001b[0;32m      3\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer, \n\u001b[0;32m      4\u001b[0m     loss_fn\u001b[38;5;241m=\u001b[39mloss, \n\u001b[0;32m      5\u001b[0m     train_dataloader\u001b[38;5;241m=\u001b[39mtrain_dataloader, \n\u001b[0;32m      6\u001b[0m     val_dataloader\u001b[38;5;241m=\u001b[39mval_dataloader, \n\u001b[0;32m      7\u001b[0m     model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/resnet34\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m      8\u001b[0m     start_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m      9\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,\n\u001b[0;32m     10\u001b[0m     early_stop_thresh\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[0;32m     11\u001b[0m )\n",
      "File \u001b[1;32m~\\AnacondaProjects\\insects_classification\\model_definition_and_training\\resnet34\\../../model_training_utils\\model_training.py:55\u001b[0m, in \u001b[0;36mtraining\u001b[1;34m(model, optimizer, loss_fn, train_dataloader, val_dataloader, model_path, epochs, early_stop_thresh, lr_decay, device, start_epoch)\u001b[0m\n\u001b[0;32m     52\u001b[0m training_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# training loop\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[0;32m     56\u001b[0m     x_batch \u001b[38;5;241m=\u001b[39m x_batch\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     57\u001b[0m     y_batch \u001b[38;5;241m=\u001b[39m y_batch\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:32\u001b[0m, in \u001b[0;36m_IterableDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index:\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 32\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_iter))\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AnacondaProjects\\insects_classification\\model_definition_and_training\\resnet34\\../../audio_preprocessing\\preprocessing_func_3.py:76\u001b[0m, in \u001b[0;36mnormalised_data_generator\u001b[1;34m(generator, mean, std)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnormalised_data_generator\u001b[39m(generator, mean, std):\n\u001b[0;32m     65\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03m    Return a generator with normalised images.\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;124;03m        generator outputing (normalised image with mean 0 and std 1, label)\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m image, label \u001b[38;5;129;01min\u001b[39;00m generator:\n\u001b[0;32m     77\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m normalise_image(image, mean, std), label\n",
      "File \u001b[1;32m~\\AnacondaProjects\\insects_classification\\model_definition_and_training\\resnet34\\../../audio_preprocessing\\preprocessing_func_3.py:44\u001b[0m, in \u001b[0;36mnon_normalised_data_generator\u001b[1;34m(paths, labels, image_preprocess_fn, mel_transform_fn, wav_max_amplitude)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (path, label) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(paths, labels):\n\u001b[0;32m     43\u001b[0m     wav \u001b[38;5;241m=\u001b[39m load_wav(path\u001b[38;5;241m=\u001b[39mpath)\n\u001b[1;32m---> 44\u001b[0m     db_mel_spec \u001b[38;5;241m=\u001b[39m mel_transform_fn(wav)\n\u001b[0;32m     45\u001b[0m     db_mel_spec \u001b[38;5;241m=\u001b[39m to_reshaped_tensor(db_mel_spec)\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m image_preprocess_fn(db_mel_spec), label\n",
      "File \u001b[1;32m~\\AnacondaProjects\\insects_classification\\model_definition_and_training\\resnet34\\../../audio_preprocessing\\preprocessing_func_3.py:26\u001b[0m, in \u001b[0;36mcalculate_melsp\u001b[1;34m(x, n_fft, hop_length)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_melsp\u001b[39m(x, n_fft\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, hop_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m):\n\u001b[1;32m---> 26\u001b[0m     stft \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(librosa\u001b[38;5;241m.\u001b[39mstft(x, n_fft\u001b[38;5;241m=\u001b[39mn_fft, hop_length\u001b[38;5;241m=\u001b[39mhop_length))\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     27\u001b[0m     log_stft \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mpower_to_db(stft)\n\u001b[0;32m     28\u001b[0m     melsp \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mmelspectrogram(S\u001b[38;5;241m=\u001b[39mlog_stft,n_mels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\librosa\\core\\spectrum.py:377\u001b[0m, in \u001b[0;36mstft\u001b[1;34m(y, n_fft, hop_length, win_length, window, center, dtype, pad_mode, out)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bl_s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, y_frames\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], n_columns):\n\u001b[0;32m    375\u001b[0m     bl_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(bl_s \u001b[38;5;241m+\u001b[39m n_columns, y_frames\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m--> 377\u001b[0m     stft_matrix[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, bl_s \u001b[38;5;241m+\u001b[39m off_start : bl_t \u001b[38;5;241m+\u001b[39m off_start] \u001b[38;5;241m=\u001b[39m fft\u001b[38;5;241m.\u001b[39mrfft(\n\u001b[0;32m    378\u001b[0m         fft_window \u001b[38;5;241m*\u001b[39m y_frames[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, bl_s:bl_t], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    379\u001b[0m     )\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stft_matrix\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mrfft\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\numpy\\fft\\_pocketfft.py:409\u001b[0m, in \u001b[0;36mrfft\u001b[1;34m(a, n, axis, norm)\u001b[0m\n\u001b[0;32m    407\u001b[0m     n \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mshape[axis]\n\u001b[0;32m    408\u001b[0m inv_norm \u001b[38;5;241m=\u001b[39m _get_forward_norm(n, norm)\n\u001b[1;32m--> 409\u001b[0m output \u001b[38;5;241m=\u001b[39m _raw_fft(a, n, axis, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m, inv_norm)\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\numpy\\fft\\_pocketfft.py:73\u001b[0m, in \u001b[0;36m_raw_fft\u001b[1;34m(a, n, axis, is_real, is_forward, inv_norm)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     a \u001b[38;5;241m=\u001b[39m swapaxes(a, axis, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 73\u001b[0m     r \u001b[38;5;241m=\u001b[39m pfi\u001b[38;5;241m.\u001b[39mexecute(a, is_real, is_forward, fct)\n\u001b[0;32m     74\u001b[0m     r \u001b[38;5;241m=\u001b[39m swapaxes(r, axis, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_training.training(\n",
    "    model=resnet_model, \n",
    "    optimizer=optimizer, \n",
    "    loss_fn=loss, \n",
    "    train_dataloader=train_dataloader, \n",
    "    val_dataloader=val_dataloader, \n",
    "    model_path=\"models/resnet34\", \n",
    "    start_epoch=5,\n",
    "    epochs=500,\n",
    "    early_stop_thresh=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec0080e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(resnet_model, 'models/resnet34/resnet34_model_input_40_256_epoch_14.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae340daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessing_func_2\n",
    "\n",
    "df_big_long_wav = pd.read_csv('data/metadata.csv')\n",
    "df_val_long_wav = df_big_long_wav[df_big_long_wav[\"subset\"]==\"validation\"]\n",
    "\n",
    "def get_df_from_class(class_num, df):\n",
    "    new_df = df[df[\"label\"] == class_num]\n",
    "    return new_df\n",
    "\n",
    "for i in range(66):\n",
    "    df_val_new = get_df_from_class(i, df_val_long_wav)\n",
    "    paths, labels = list(df_val_new[\"path\"]), list(df_val_new[\"label\"])\n",
    "    non_normalised_generator = preprocessing_func_2.non_normalised_data_generator(\n",
    "        paths=paths, \n",
    "        labels=labels,\n",
    "#         image_preprocess_fn=resize_function(output_shape=(40,256)),\n",
    "#         mel_transform_fn=calculate_melsp,\n",
    "    )\n",
    "    normalised_generator = preprocessing_func_2.normalised_data_generator(\n",
    "        non_normalised_generator, mean, std)\n",
    "    print(f\"we are now in class {i}\")\n",
    "    final_pred = model_eval.evaluation(resnet_model, normalised_generator)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e565ff2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_big_long_wav = pd.read_csv('data/metadata.csv')\n",
    "df_val_long_wav = df_big_long_wav[df_big_long_wav[\"subset\"]==\"validation\"]\n",
    "\n",
    "\n",
    "paths, labels = list(df_val_long_wav[\"path\"]), list(df_val_long_wav[\"label\"])\n",
    "non_normalised_generator = preprocessing_func_2.non_normalised_data_generator(\n",
    "    paths=paths, \n",
    "    labels=labels,\n",
    "#     image_preprocess_fn=resize_function(output_shape=(40,256)),\n",
    "#     mel_transform_fn=calculate_melsp,\n",
    ")\n",
    "normalised_generator = preprocessing_func_2.normalised_data_generator(non_normalised_generator, mean, std)\n",
    "model_eval.evaluation(resnet_model, normalised_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8275773",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
